"""
LLM å®¢æˆ·ç«¯æ¨¡å—

æä¾›ä¸åŸæœ‰ LLMClient å…¼å®¹çš„æ¥å£ï¼Œæ–¹ä¾¿è¿ç§»

ä½œè€…: Ai_Test_Agent Team
ç‰ˆæœ¬: 1.0
"""
import json
import logging
from typing import Any, Dict, List, Optional

from .manager import get_active_llm_config, model_config_manager
from .factory import create_llm_provider
from .base import BaseLLMProvider

logger = logging.getLogger(__name__)


class LLMClient:
    """
    LLM å®¢æˆ·ç«¯
    
    å…¼å®¹åŸæœ‰ LLMClient æ¥å£ï¼Œä½¿ç”¨æ–°çš„ Provider æ¶æ„
    """
    
    def __init__(self):
        self._provider: Optional[BaseLLMProvider] = None
        self._config: Optional[Dict] = None
    
    def _get_config(self) -> Dict:
        """è·å–æ¨¡å‹é…ç½®"""
        if self._config is None:
            try:
                self._config = get_active_llm_config()
            except Exception as e:
                logger.error(f"[LLMClient] è·å–æ•°æ®åº“æ¨¡å‹é…ç½®å¤±è´¥: {e}")
                raise
        return self._config
    
    def _ensure_provider(self):
        """ç¡®ä¿ Provider å·²åˆå§‹åŒ–"""
        if self._provider is None:
            config = self._get_config()
            self._provider = create_llm_provider(
                provider=config['provider'],
                model_name=config['model_name'],
                api_key=config['api_key'],
                base_url=config['base_url'],
                temperature=config.get('temperature', 0.0),
                max_tokens=config.get('max_tokens', 128000),
            )
    
    def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 4000,
        response_format: Optional[Dict[str, str]] = None,
        source: str = "chat",
        session_id: int = None,
    ) -> str:
        """
        å‘é€èŠå¤©è¯·æ±‚ï¼ˆå¸¦è‡ªåŠ¨åˆ‡æ¢å’Œè¯¦ç»† Token ç»Ÿè®¡ï¼‰
        """
        import time as _time
        self._ensure_provider()
        start_ms = int(_time.time() * 1000)
        
        try:
            response = self._provider.chat(
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                response_format=response_format
            )
            
            duration_ms = int(_time.time() * 1000) - start_ms

            # æ›´æ–° token ä½¿ç”¨é‡ï¼ˆå¢å¼ºç‰ˆï¼‰
            model_config_manager.increment_token_usage(
                tokens=response.total_tokens,
                prompt_tokens=response.prompt_tokens,
                completion_tokens=response.completion_tokens,
                source=source,
                session_id=session_id,
                success=True,
                duration_ms=duration_ms,
            )
            
            return response.content
            
        except Exception as e:
            duration_ms = int(_time.time() * 1000) - start_ms
            logger.error(f"[LLMClient] è¯·æ±‚å¤±è´¥: {e}")

            # è®°å½•å¤±è´¥çš„ token ä½¿ç”¨
            model_config_manager.increment_token_usage(
                tokens=0,
                prompt_tokens=0,
                completion_tokens=0,
                source=source,
                session_id=session_id,
                success=False,
                error_type=str(type(e).__name__),
                duration_ms=duration_ms,
            )

            # å°è¯•è‡ªåŠ¨åˆ‡æ¢
            try:
                from llm.auto_switch import get_auto_switcher, classify_failure_reason
                switcher = get_auto_switcher()
                # ç¡®ä¿ profiles å·²åŠ è½½ï¼ˆé¦–æ¬¡è°ƒç”¨æ—¶å¯èƒ½ä¸ºç©ºï¼‰
                if not switcher._profiles:
                    switcher.load_profiles_from_db()
                if switcher.enabled and self._config and len(switcher._profiles) > 1:
                    reason = classify_failure_reason(e)
                    current_id = self._config.get('id', 0)
                    new_id = switcher.mark_failure(current_id, reason)
                    if new_id and new_id != current_id:
                        logger.info(f"[LLMClient] ğŸ”„ è‡ªåŠ¨åˆ‡æ¢: ID={current_id} â†’ ID={new_id}ï¼Œé‡è¯•è¯·æ±‚")
                        self.refresh()
                        self._ensure_provider()
                        response = self._provider.chat(
                            messages=messages,
                            temperature=temperature,
                            max_tokens=max_tokens,
                            response_format=response_format
                        )
                        retry_duration = int(_time.time() * 1000) - start_ms
                        model_config_manager.increment_token_usage(
                            tokens=response.total_tokens,
                            prompt_tokens=response.prompt_tokens,
                            completion_tokens=response.completion_tokens,
                            source=source,
                            session_id=session_id,
                            success=True,
                            duration_ms=retry_duration,
                        )
                        return response.content
                    else:
                        logger.warning(f"[LLMClient] âŒ æ²¡æœ‰å¯ç”¨çš„å¤‡é€‰æ¨¡å‹ (current={current_id}, new={new_id})")
            except Exception as retry_err:
                logger.error(f"[LLMClient] è‡ªåŠ¨åˆ‡æ¢é‡è¯•ä¹Ÿå¤±è´¥: {retry_err}")

            raise
    
    async def achat(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 4000,
        response_format: Optional[Dict[str, str]] = None,
        source: str = "chat",
        session_id: int = None,
    ) -> str:
        """å¼‚æ­¥èŠå¤©è¯·æ±‚ï¼ˆå¸¦è‡ªåŠ¨åˆ‡æ¢å’Œè¯¦ç»† Token ç»Ÿè®¡ï¼‰"""
        import time as _time
        self._ensure_provider()
        start_ms = int(_time.time() * 1000)
        
        try:
            response = await self._provider.achat(
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                response_format=response_format
            )
            
            duration_ms = int(_time.time() * 1000) - start_ms

            model_config_manager.increment_token_usage(
                tokens=response.total_tokens,
                prompt_tokens=response.prompt_tokens,
                completion_tokens=response.completion_tokens,
                source=source,
                session_id=session_id,
                success=True,
                duration_ms=duration_ms,
            )
            
            return response.content
            
        except Exception as e:
            duration_ms = int(_time.time() * 1000) - start_ms
            logger.error(f"[LLMClient] å¼‚æ­¥è¯·æ±‚å¤±è´¥: {e}")

            model_config_manager.increment_token_usage(
                tokens=0, prompt_tokens=0, completion_tokens=0,
                source=source, session_id=session_id,
                success=False, error_type=str(type(e).__name__),
                duration_ms=duration_ms,
            )

            # å°è¯•è‡ªåŠ¨åˆ‡æ¢
            try:
                from llm.auto_switch import get_auto_switcher, classify_failure_reason
                switcher = get_auto_switcher()
                # ç¡®ä¿ profiles å·²åŠ è½½ï¼ˆé¦–æ¬¡è°ƒç”¨æ—¶å¯èƒ½ä¸ºç©ºï¼‰
                if not switcher._profiles:
                    switcher.load_profiles_from_db()
                if switcher.enabled and self._config and len(switcher._profiles) > 1:
                    reason = classify_failure_reason(e)
                    current_id = self._config.get('id', 0)
                    new_id = switcher.mark_failure(current_id, reason)
                    if new_id and new_id != current_id:
                        logger.info(f"[LLMClient] ğŸ”„ è‡ªåŠ¨åˆ‡æ¢: ID={current_id} â†’ ID={new_id}ï¼Œé‡è¯•å¼‚æ­¥è¯·æ±‚")
                        self.refresh()
                        self._ensure_provider()
                        response = await self._provider.achat(
                            messages=messages,
                            temperature=temperature,
                            max_tokens=max_tokens,
                            response_format=response_format
                        )
                        retry_duration = int(_time.time() * 1000) - start_ms
                        model_config_manager.increment_token_usage(
                            tokens=response.total_tokens,
                            prompt_tokens=response.prompt_tokens,
                            completion_tokens=response.completion_tokens,
                            source=source, session_id=session_id,
                            success=True, duration_ms=retry_duration,
                        )
                        return response.content
                    else:
                        logger.warning(f"[LLMClient] âŒ æ²¡æœ‰å¯ç”¨çš„å¤‡é€‰æ¨¡å‹ (current={current_id}, new={new_id})")
            except Exception as retry_err:
                logger.error(f"[LLMClient] è‡ªåŠ¨åˆ‡æ¢é‡è¯•ä¹Ÿå¤±è´¥: {retry_err}")

            raise
    
    def generate_test_cases(
        self,
        requirement: str,
        count: int = 3,
        priority: str = "3"
    ) -> Dict[str, Any]:
        """
        æ ¹æ®éœ€æ±‚ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
        
        Args:
            requirement: ç”¨æˆ·éœ€æ±‚æˆ–æµ‹è¯•ç‚¹
            count: ç”Ÿæˆæ•°é‡
            priority: é»˜è®¤ä¼˜å…ˆçº§
        
        Returns:
            ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹æ•°æ®
        """
        template_fields = [
            "module", "title", "precondition", "steps", "expected",
            "keywords", "priority", "case_type", "stage"
        ]
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è½¯ä»¶æµ‹è¯•å·¥ç¨‹å¸ˆï¼Œæ“…é•¿ç¼–å†™æµ‹è¯•ç”¨ä¾‹ã€‚
è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„éœ€æ±‚ï¼Œç”Ÿæˆç»“æ„åŒ–çš„æµ‹è¯•ç”¨ä¾‹ã€‚

è¾“å‡ºæ ¼å¼è¦æ±‚:
- å¿…é¡»è¿”å›æœ‰æ•ˆçš„ JSON å¯¹è±¡
- åŒ…å« test_cases æ•°ç»„
- æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹åŒ…å«: module, title, precondition, steps, expected, keywords, priority, case_type, stage"""
        
        user_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹éœ€æ±‚ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹:

éœ€æ±‚: {requirement}

è¯·ç”ŸæˆåŒ…å«ä»¥ä¸‹å­—æ®µçš„æµ‹è¯•ç”¨ä¾‹: {', '.join(template_fields)}

ä»¥ JSON æ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚:
{{
    "test_cases": [
        {{
            "module": "æ¨¡å—å",
            "title": "ç”¨ä¾‹æ ‡é¢˜",
            "precondition": "å‰ç½®æ¡ä»¶",
            "steps": ["æ­¥éª¤1", "æ­¥éª¤2"],
            "expected": "é¢„æœŸç»“æœ",
            "keywords": "å…³é”®è¯",
            "priority": "3",
            "case_type": "åŠŸèƒ½æµ‹è¯•",
            "stage": "ç³»ç»Ÿæµ‹è¯•"
        }}
    ]
}}"""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        try:
            response = self.chat(
                messages=messages,
                temperature=0.7,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response)
            return {
                "success": True,
                **result
            }
        except Exception as e:
            return {
                "success": False,
                "message": str(e),
                "test_cases": []
            }
    
    def generate_test_report(
        self,
        test_results: List[Dict[str, Any]],
        summary: Dict[str, Any],
        format_type: str = "markdown"
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        
        Args:
            test_results: æµ‹è¯•ç»“æœåˆ—è¡¨
            summary: æµ‹è¯•ç»Ÿè®¡æ‘˜è¦
            format_type: æŠ¥å‘Šæ ¼å¼
        
        Returns:
            ç”Ÿæˆçš„æŠ¥å‘Š
        """
        total_cases = summary.get('total', len(test_results))
        is_single_case = total_cases == 1
        
        test_results_json = json.dumps(test_results, ensure_ascii=False, indent=2)
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æµ‹è¯•æŠ¥å‘Šç”Ÿæˆä¸“å®¶ã€‚
è¯·æ ¹æ®æµ‹è¯•ç»“æœç”Ÿæˆæ¸…æ™°ã€ä¸“ä¸šçš„æµ‹è¯•æŠ¥å‘Šã€‚"""
        
        if is_single_case:
            user_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹æµ‹è¯•ç»“æœç”Ÿæˆæµ‹è¯•æŠ¥å‘Š:

æµ‹è¯•ç»“æœ:
{test_results_json}

è¯·ç”Ÿæˆ {format_type.upper()} æ ¼å¼çš„æµ‹è¯•æŠ¥å‘Šã€‚"""
        else:
            user_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹æ‰¹é‡æµ‹è¯•ç»“æœç”Ÿæˆæµ‹è¯•æŠ¥å‘Š:

æµ‹è¯•ç»Ÿè®¡:
- æ€»ç”¨ä¾‹æ•°: {summary.get('total', 0)}
- é€šè¿‡æ•°: {summary.get('pass', 0)}
- å¤±è´¥æ•°: {summary.get('fail', 0)}
- æ‰§è¡Œè€—æ—¶: {summary.get('duration', 0)} ç§’

æµ‹è¯•ç»“æœè¯¦æƒ…:
{test_results_json}

è¯·ç”Ÿæˆ {format_type.upper()} æ ¼å¼çš„æµ‹è¯•æŠ¥å‘Šã€‚"""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        try:
            response = self.chat(
                messages=messages,
                temperature=0.3
            )
            
            return {
                "success": True,
                "content": response
            }
        except Exception as e:
            return {
                "success": False,
                "message": str(e),
                "content": ""
            }
    
    def analyze_bug(self, analysis_prompt: str) -> Dict[str, Any]:
        """
        åˆ†æ Bug
        
        Args:
            analysis_prompt: Bug åˆ†ææç¤º
        
        Returns:
            Bug åˆ†æç»“æœ
        """
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Bug åˆ†æä¸“å®¶ã€‚
è¯·åˆ†ææµ‹è¯•å¤±è´¥çš„åŸå› ï¼Œå¹¶è¿”å›ç»“æ„åŒ–çš„ Bug æŠ¥å‘Šã€‚

è¿”å› JSON æ ¼å¼:
{
    "error_type": "é”™è¯¯ç±»å‹ï¼ˆåŠŸèƒ½é”™è¯¯/è®¾è®¡ç¼ºé™·/å®‰å…¨é—®é¢˜/ç³»ç»Ÿé”™è¯¯ï¼‰",
    "severity_level": "ä¸¥é‡ç¨‹åº¦ï¼ˆä¸€çº§/äºŒçº§/ä¸‰çº§/å››çº§ï¼‰",
    "actual_result": "å®é™…ç»“æœæè¿°",
    "result_feedback": "é—®é¢˜åˆ†æå’Œå»ºè®®"
}"""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": analysis_prompt}
        ]
        
        try:
            response = self.chat(
                messages=messages,
                temperature=0.1,
                max_tokens=2000
            )
            
            # æ¸…ç† markdown ä»£ç å—
            cleaned_response = response.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]
            if cleaned_response.startswith('```'):
                cleaned_response = cleaned_response[3:]
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]
            cleaned_response = cleaned_response.strip()
            
            bug_data = json.loads(cleaned_response)
            
            return {
                "success": True,
                "data": bug_data
            }
            
        except json.JSONDecodeError as e:
            logger.warning(f"[LLMClient] JSON è§£æå¤±è´¥: {e}")
            return {
                "success": True,
                "data": {
                    "error_type": "ç³»ç»Ÿé”™è¯¯",
                    "severity_level": "ä¸€çº§",
                    "actual_result": response if 'response' in dir() else "å“åº”æ ¼å¼å¼‚å¸¸",
                    "result_feedback": "LLM è¿”å›æ ¼å¼å¼‚å¸¸ï¼Œæ— æ³•è§£æ"
                }
            }
        except Exception as e:
            return {
                "success": False,
                "message": str(e),
                "data": {}
            }
    
    def refresh(self):
        """åˆ·æ–°é…ç½®å’Œ Provider"""
        self._config = None
        self._provider = None
        model_config_manager.refresh_config()

    def parse_json_response(self, content: str) -> dict:
        """
        ä½¿ç”¨å½“å‰ Provider çš„ JSON è§£æå™¨è§£æ LLM å“åº”

        è‡ªåŠ¨æ ¹æ®å½“å‰æ´»è·ƒçš„æ¨¡å‹ Provider é€‰æ‹©æœ€åˆé€‚çš„è§£æç­–ç•¥

        Args:
            content: LLM åŸå§‹å“åº”æ–‡æœ¬

        Returns:
            è§£æåçš„ dict
        """
        self._ensure_provider()
        return self._provider.parse_json_response(content)


# å…¨å±€å®ä¾‹
_llm_client_instance: Optional[LLMClient] = None


def get_llm_client() -> LLMClient:
    """è·å– LLM å®¢æˆ·ç«¯å®ä¾‹ï¼ˆå•ä¾‹ï¼‰"""
    global _llm_client_instance
    if _llm_client_instance is None:
        _llm_client_instance = LLMClient()
    return _llm_client_instance
