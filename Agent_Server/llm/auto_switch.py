"""
æ¨¡å‹è‡ªåŠ¨åˆ‡æ¢æ¨¡å— (Auth Profile Rotation)

å€Ÿé‰´ OpenClaw çš„ Auth Profile è½®æ¢æœºåˆ¶ï¼š
- ç»´æŠ¤å¤šä¸ªæ¨¡å‹/API Key çš„å¥åº·çŠ¶æ€
- å½“é‡åˆ°é™æµ(429)/è®¤è¯å¤±è´¥/è¶…æ—¶æ—¶ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯ç”¨æ¨¡å‹
- å¸¦å†·å´æ£€æµ‹ï¼šå¤±è´¥çš„æ¨¡å‹è¿›å…¥å†·å´æœŸï¼Œå†·å´ç»“æŸåè‡ªåŠ¨æ¢å¤
- Token ä½¿ç”¨é‡ç»Ÿè®¡å’Œåˆ©ç”¨ç‡è¿½è¸ª

ä½œè€…: Ai_Test_Agent Team
ç‰ˆæœ¬: 2.0
"""
import time
import logging
import asyncio
from typing import Any, Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime

logger = logging.getLogger(__name__)


class FailureReason(str, Enum):
    """å¤±è´¥åŸå› åˆ†ç±»"""
    RATE_LIMIT = "rate_limit"        # 429 é™æµ
    AUTH_FAILURE = "auth_failure"     # è®¤è¯å¤±è´¥ (401/403)
    TIMEOUT = "timeout"              # è¶…æ—¶
    SERVER_ERROR = "server_error"    # æœåŠ¡ç«¯é”™è¯¯ (500+)
    QUOTA_EXCEEDED = "quota_exceeded"  # é…é¢è€—å°½
    CONTEXT_OVERFLOW = "context_overflow"  # ä¸Šä¸‹æ–‡æº¢å‡º
    UNKNOWN = "unknown"


@dataclass
class ModelProfile:
    """æ¨¡å‹é…ç½®æ¡£æ¡ˆï¼ˆå¯¹åº”æ•°æ®åº“ llm_models è¡¨çš„ä¸€è¡Œï¼‰"""
    model_id: int
    model_name: str
    provider: str
    api_key: str
    base_url: str
    priority: int = 1
    utilization: int = 100  # åˆ©ç”¨ç‡ç™¾åˆ†æ¯”

    # è¿è¡Œæ—¶çŠ¶æ€ï¼ˆä¸æŒä¹…åŒ–ï¼‰
    failure_count: int = 0
    last_failure_time: float = 0.0
    last_failure_reason: Optional[FailureReason] = None
    cooldown_until: float = 0.0  # å†·å´ç»“æŸæ—¶é—´æˆ³
    consecutive_failures: int = 0
    last_success_time: float = 0.0
    total_requests: int = 0
    total_tokens_used: int = 0

    @property
    def is_cooling_down(self) -> bool:
        return time.time() < self.cooldown_until

    @property
    def is_available(self) -> bool:
        return not self.is_cooling_down and self.utilization > 0

    @property
    def cooldown_remaining(self) -> float:
        remaining = self.cooldown_until - time.time()
        return max(0, remaining)


# å†·å´æ—¶é—´é…ç½®ï¼ˆç§’ï¼‰
COOLDOWN_CONFIG = {
    FailureReason.RATE_LIMIT: {
        "base": 60,       # åŸºç¡€å†·å´ 60 ç§’
        "max": 600,       # æœ€å¤§å†·å´ 10 åˆ†é’Ÿ
        "multiplier": 2,  # æŒ‡æ•°é€€é¿å€æ•°
    },
    FailureReason.AUTH_FAILURE: {
        "base": 300,      # è®¤è¯å¤±è´¥å†·å´ 5 åˆ†é’Ÿ
        "max": 3600,      # æœ€å¤§ 1 å°æ—¶
        "multiplier": 3,
    },
    FailureReason.TIMEOUT: {
        "base": 30,
        "max": 300,
        "multiplier": 2,
    },
    FailureReason.SERVER_ERROR: {
        "base": 30,
        "max": 300,
        "multiplier": 2,
    },
    FailureReason.QUOTA_EXCEEDED: {
        "base": 600,      # é…é¢è€—å°½å†·å´ 10 åˆ†é’Ÿ
        "max": 7200,      # æœ€å¤§ 2 å°æ—¶
        "multiplier": 3,
    },
    FailureReason.CONTEXT_OVERFLOW: {
        "base": 0,        # ä¸Šä¸‹æ–‡æº¢å‡ºä¸éœ€è¦å†·å´ï¼ˆæ¢æ¨¡å‹å³å¯ï¼‰
        "max": 0,
        "multiplier": 1,
    },
    FailureReason.UNKNOWN: {
        "base": 30,
        "max": 300,
        "multiplier": 2,
    },
}


def classify_failure_reason(error: Exception) -> FailureReason:
    """
    æ ¹æ®å¼‚å¸¸ç±»å‹åˆ†ç±»å¤±è´¥åŸå› 
    å€Ÿé‰´ OpenClaw çš„ classifyFailoverReason
    """
    error_msg = str(error).lower()

    # 429 é™æµ
    if any(kw in error_msg for kw in [
        "429", "rate limit", "rate_limit", "too many requests",
        "quota", "exceeded", "throttl"
    ]):
        if any(kw in error_msg for kw in ["quota", "exceeded", "billing"]):
            return FailureReason.QUOTA_EXCEEDED
        return FailureReason.RATE_LIMIT

    # è®¤è¯å¤±è´¥
    if any(kw in error_msg for kw in [
        "401", "403", "unauthorized", "forbidden",
        "invalid api key", "invalid_api_key", "authentication"
    ]):
        return FailureReason.AUTH_FAILURE

    # è¶…æ—¶
    if any(kw in error_msg for kw in [
        "timeout", "timed out", "time out", "deadline"
    ]):
        return FailureReason.TIMEOUT

    # æœåŠ¡ç«¯é”™è¯¯
    if any(kw in error_msg for kw in [
        "500", "502", "503", "504", "internal server error",
        "bad gateway", "service unavailable", "gateway timeout"
    ]):
        return FailureReason.SERVER_ERROR

    # ä¸Šä¸‹æ–‡æº¢å‡º
    if any(kw in error_msg for kw in [
        "context length", "context_length", "token limit",
        "maximum context", "too long", "max_tokens"
    ]):
        return FailureReason.CONTEXT_OVERFLOW

    return FailureReason.UNKNOWN


def _calculate_cooldown(reason: FailureReason, consecutive_failures: int) -> float:
    """è®¡ç®—å†·å´æ—¶é—´ï¼ˆæŒ‡æ•°é€€é¿ï¼‰"""
    config = COOLDOWN_CONFIG.get(reason, COOLDOWN_CONFIG[FailureReason.UNKNOWN])
    base = config["base"]
    max_cd = config["max"]
    multiplier = config["multiplier"]

    if base == 0:
        return 0

    cooldown = base * (multiplier ** min(consecutive_failures - 1, 5))
    return min(cooldown, max_cd)


class ModelAutoSwitcher:
    """
    æ¨¡å‹è‡ªåŠ¨åˆ‡æ¢å™¨

    æ ¸å¿ƒé€»è¾‘ï¼ˆå€Ÿé‰´ OpenClawï¼‰ï¼š
    1. æŒ‰ä¼˜å…ˆçº§ç»´æŠ¤æ¨¡å‹åˆ—è¡¨
    2. è¯·æ±‚å¤±è´¥æ—¶æ ‡è®°å¤±è´¥ã€è®¡ç®—å†·å´æ—¶é—´
    3. è‡ªåŠ¨åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯ç”¨æ¨¡å‹
    4. è¯·æ±‚æˆåŠŸæ—¶æ ‡è®°æ¢å¤
    5. æ‰€æœ‰æ¨¡å‹éƒ½ä¸å¯ç”¨æ—¶ï¼Œé€‰æ‹©å†·å´æ—¶é—´æœ€çŸ­çš„ç­‰å¾…
    """

    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return
        self._profiles: Dict[int, ModelProfile] = {}
        self._current_model_id: Optional[int] = None
        self._enabled: bool = True
        self._switch_history: List[Dict] = []  # åˆ‡æ¢å†å²è®°å½•
        self._lock = asyncio.Lock()
        self._initialized = True
        logger.info("[AutoSwitch] æ¨¡å‹è‡ªåŠ¨åˆ‡æ¢å™¨å·²åˆå§‹åŒ–")

    def load_profiles_from_db(self, db=None):
        """ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰æ¨¡å‹é…ç½®"""
        close_db = False
        if db is None:
            from database.connection import SessionLocal
            db = SessionLocal()
            close_db = True

        try:
            from database.connection import LLMModel
            models = db.query(LLMModel).order_by(LLMModel.priority).all()

            for model in models:
                profile = ModelProfile(
                    model_id=model.id,
                    model_name=model.model_name,
                    provider=model.provider or "openai",
                    api_key=model.api_key,
                    base_url=model.base_url or "",
                    priority=model.priority or 1,
                    utilization=model.utilization or 100,
                )
                # ä¿ç•™å·²æœ‰çš„è¿è¡Œæ—¶çŠ¶æ€
                if model.id in self._profiles:
                    old = self._profiles[model.id]
                    profile.failure_count = old.failure_count
                    profile.last_failure_time = old.last_failure_time
                    profile.last_failure_reason = old.last_failure_reason
                    profile.cooldown_until = old.cooldown_until
                    profile.consecutive_failures = old.consecutive_failures
                    profile.last_success_time = old.last_success_time
                    profile.total_requests = old.total_requests
                    profile.total_tokens_used = old.total_tokens_used

                self._profiles[model.id] = profile

                if model.is_active == 1:
                    self._current_model_id = model.id

            logger.info(
                f"[AutoSwitch] å·²åŠ è½½ {len(self._profiles)} ä¸ªæ¨¡å‹é…ç½®, "
                f"å½“å‰æ´»åŠ¨: {self._current_model_id}"
            )
        except Exception as e:
            logger.error(f"[AutoSwitch] åŠ è½½æ¨¡å‹é…ç½®å¤±è´¥: {e}")
        finally:
            if close_db:
                db.close()

    @property
    def enabled(self) -> bool:
        return self._enabled

    @enabled.setter
    def enabled(self, value: bool):
        self._enabled = value
        logger.info(f"[AutoSwitch] è‡ªåŠ¨åˆ‡æ¢å·²{'å¼€å¯' if value else 'å…³é—­'}")

    @property
    def current_profile(self) -> Optional[ModelProfile]:
        if self._current_model_id is None:
            return None
        return self._profiles.get(self._current_model_id)

    def get_all_profiles_status(self) -> List[Dict]:
        """è·å–æ‰€æœ‰æ¨¡å‹çš„çŠ¶æ€ä¿¡æ¯"""
        result = []
        for mid, p in sorted(self._profiles.items(), key=lambda x: x[1].priority):
            result.append({
                "model_id": p.model_id,
                "model_name": p.model_name,
                "provider": p.provider,
                "priority": p.priority,
                "utilization": p.utilization,
                "is_current": mid == self._current_model_id,
                "is_available": p.is_available,
                "is_cooling_down": p.is_cooling_down,
                "cooldown_remaining": round(p.cooldown_remaining, 1),
                "failure_count": p.failure_count,
                "consecutive_failures": p.consecutive_failures,
                "last_failure_reason": p.last_failure_reason.value if p.last_failure_reason else None,
                "total_requests": p.total_requests,
                "total_tokens_used": p.total_tokens_used,
            })
        return result

    def get_switch_history(self, limit: int = 20) -> List[Dict]:
        """è·å–åˆ‡æ¢å†å²"""
        return self._switch_history[-limit:]

    def mark_failure(self, model_id: int, reason: FailureReason) -> Optional[int]:
        """
        æ ‡è®°æ¨¡å‹å¤±è´¥ï¼Œè¿”å›åˆ‡æ¢åˆ°çš„æ–°æ¨¡å‹ IDï¼ˆå¦‚æœå‘ç”Ÿäº†åˆ‡æ¢ï¼‰

        å€Ÿé‰´ OpenClaw çš„ markAuthProfileFailure
        """
        profile = self._profiles.get(model_id)
        if not profile:
            return None

        profile.failure_count += 1
        profile.consecutive_failures += 1
        profile.last_failure_time = time.time()
        profile.last_failure_reason = reason

        # è®¡ç®—å†·å´æ—¶é—´
        cooldown_seconds = _calculate_cooldown(reason, profile.consecutive_failures)
        if cooldown_seconds > 0:
            profile.cooldown_until = time.time() + cooldown_seconds

        logger.warning(
            f"[AutoSwitch] æ¨¡å‹ {profile.model_name}(ID={model_id}) å¤±è´¥: "
            f"reason={reason.value}, consecutive={profile.consecutive_failures}, "
            f"cooldown={cooldown_seconds:.0f}s"
        )

        # å¦‚æœè‡ªåŠ¨åˆ‡æ¢å¼€å¯ä¸”å½“å‰æ¨¡å‹å°±æ˜¯å¤±è´¥çš„æ¨¡å‹ï¼Œå°è¯•åˆ‡æ¢
        if self._enabled and model_id == self._current_model_id:
            next_id = self._find_next_available()
            if next_id and next_id != model_id:
                return self._do_switch(next_id, reason=f"failover from {profile.model_name}")
        return None

    def mark_success(self, model_id: int, tokens_used: int = 0):
        """
        æ ‡è®°æ¨¡å‹æˆåŠŸ

        å€Ÿé‰´ OpenClaw çš„ markAuthProfileGood
        """
        profile = self._profiles.get(model_id)
        if not profile:
            return

        profile.consecutive_failures = 0
        profile.last_success_time = time.time()
        profile.total_requests += 1
        if tokens_used > 0:
            profile.total_tokens_used += tokens_used

    def _find_next_available(self) -> Optional[int]:
        """
        æŸ¥æ‰¾ä¸‹ä¸€ä¸ªå¯ç”¨æ¨¡å‹ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰

        ç­–ç•¥ï¼š
        1. ä¼˜å…ˆé€‰æ‹© is_available ä¸”ä¼˜å…ˆçº§æœ€é«˜çš„
        2. å¦‚æœéƒ½åœ¨å†·å´ä¸­ï¼Œé€‰æ‹©å†·å´æ—¶é—´æœ€çŸ­çš„
        """
        sorted_profiles = sorted(
            self._profiles.values(),
            key=lambda p: (p.priority, p.failure_count)
        )

        # ç¬¬ä¸€è½®ï¼šæ‰¾å¯ç”¨çš„
        for p in sorted_profiles:
            if p.is_available and p.model_id != self._current_model_id:
                return p.model_id

        # ç¬¬äºŒè½®ï¼šæ‰¾å†·å´æ—¶é—´æœ€çŸ­çš„ï¼ˆæ’é™¤å½“å‰ï¼‰
        cooling = [
            p for p in sorted_profiles
            if p.model_id != self._current_model_id and p.utilization > 0
        ]
        if cooling:
            best = min(cooling, key=lambda p: p.cooldown_until)
            return best.model_id

        # æ‰€æœ‰æ¨¡å‹éƒ½ä¸å¯ç”¨ï¼Œè¿”å›å½“å‰çš„
        return self._current_model_id

    def _do_switch(self, new_model_id: int, reason: str = "") -> int:
        """æ‰§è¡Œæ¨¡å‹åˆ‡æ¢"""
        old_id = self._current_model_id
        old_name = self._profiles[old_id].model_name if old_id and old_id in self._profiles else "None"
        new_name = self._profiles[new_model_id].model_name

        self._current_model_id = new_model_id

        # è®°å½•åˆ‡æ¢å†å²
        record = {
            "time": datetime.now().isoformat(),
            "from_id": old_id,
            "from_name": old_name,
            "to_id": new_model_id,
            "to_name": new_name,
            "reason": reason,
        }
        self._switch_history.append(record)
        if len(self._switch_history) > 100:
            self._switch_history = self._switch_history[-50:]

        logger.info(
            f"[AutoSwitch] ğŸ”„ æ¨¡å‹åˆ‡æ¢: {old_name} â†’ {new_name} (reason: {reason})"
        )

        # åŒæ­¥åˆ°æ•°æ®åº“
        self._sync_active_to_db(new_model_id)

        return new_model_id

    def _sync_active_to_db(self, model_id: int):
        """å°†æ´»åŠ¨æ¨¡å‹åŒæ­¥åˆ°æ•°æ®åº“"""
        try:
            from database.connection import SessionLocal, LLMModel
            db = SessionLocal()
            try:
                db.query(LLMModel).update({LLMModel.is_active: 0})
                model = db.query(LLMModel).filter(LLMModel.id == model_id).first()
                if model:
                    model.is_active = 1
                    model.status = model.model_name
                    model.updated_at = datetime.now()
                db.commit()

                # åˆ·æ–° ModelConfigManager ç¼“å­˜
                from llm.manager import model_config_manager
                model_config_manager.refresh_config(db)

                logger.info(f"[AutoSwitch] æ•°æ®åº“å·²åŒæ­¥: model_id={model_id} å·²æ¿€æ´»")
            finally:
                db.close()
        except Exception as e:
            logger.error(f"[AutoSwitch] åŒæ­¥æ•°æ®åº“å¤±è´¥: {e}")

    def force_switch(self, model_id: int) -> bool:
        """æ‰‹åŠ¨å¼ºåˆ¶åˆ‡æ¢åˆ°æŒ‡å®šæ¨¡å‹"""
        if model_id not in self._profiles:
            return False
        self._do_switch(model_id, reason="manual switch")
        return True

    def reset_profile(self, model_id: int):
        """é‡ç½®æ¨¡å‹çš„å¤±è´¥çŠ¶æ€"""
        profile = self._profiles.get(model_id)
        if profile:
            profile.failure_count = 0
            profile.consecutive_failures = 0
            profile.cooldown_until = 0
            profile.last_failure_reason = None
            logger.info(f"[AutoSwitch] å·²é‡ç½®æ¨¡å‹ {profile.model_name} çš„çŠ¶æ€")

    def reset_all(self):
        """é‡ç½®æ‰€æœ‰æ¨¡å‹çŠ¶æ€"""
        for p in self._profiles.values():
            p.failure_count = 0
            p.consecutive_failures = 0
            p.cooldown_until = 0
            p.last_failure_reason = None
        logger.info("[AutoSwitch] å·²é‡ç½®æ‰€æœ‰æ¨¡å‹çŠ¶æ€")

    async def call_with_failover(
        self,
        call_fn,
        max_retries: int = 3,
        **kwargs
    ) -> Tuple[Any, int]:
        """
        å¸¦è‡ªåŠ¨åˆ‡æ¢çš„è°ƒç”¨å°è£…

        Args:
            call_fn: å¼‚æ­¥è°ƒç”¨å‡½æ•°ï¼Œç­¾å async def fn(profile: ModelProfile, **kwargs) -> result
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            **kwargs: ä¼ é€’ç»™ call_fn çš„é¢å¤–å‚æ•°

        Returns:
            (result, model_id) å…ƒç»„

        Raises:
            æœ€åä¸€æ¬¡å¤±è´¥çš„å¼‚å¸¸
        """
        if not self._profiles:
            self.load_profiles_from_db()

        last_error = None
        tried_models = set()

        for attempt in range(max_retries):
            profile = self.current_profile
            if not profile:
                raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹é…ç½®")

            # å¦‚æœå½“å‰æ¨¡å‹åœ¨å†·å´ä¸­ä¸”è‡ªåŠ¨åˆ‡æ¢å¼€å¯ï¼Œå…ˆåˆ‡æ¢
            if profile.is_cooling_down and self._enabled:
                next_id = self._find_next_available()
                if next_id and next_id != profile.model_id:
                    self._do_switch(next_id, reason="current model cooling down")
                    profile = self.current_profile
                elif profile.is_cooling_down:
                    # æ‰€æœ‰æ¨¡å‹éƒ½åœ¨å†·å´ï¼Œç­‰å¾…æœ€çŸ­å†·å´æ—¶é—´
                    wait_time = min(
                        p.cooldown_remaining for p in self._profiles.values()
                        if p.cooldown_remaining > 0
                    ) if any(p.cooldown_remaining > 0 for p in self._profiles.values()) else 5
                    wait_time = min(wait_time, 30)  # æœ€å¤šç­‰ 30 ç§’
                    logger.info(f"[AutoSwitch] æ‰€æœ‰æ¨¡å‹å†·å´ä¸­ï¼Œç­‰å¾… {wait_time:.1f}s")
                    await asyncio.sleep(wait_time)

            if not profile:
                raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹é…ç½®")

            tried_models.add(profile.model_id)

            try:
                result = await call_fn(profile, **kwargs)
                self.mark_success(profile.model_id)
                return result, profile.model_id

            except Exception as e:
                last_error = e
                reason = classify_failure_reason(e)
                new_id = self.mark_failure(profile.model_id, reason)

                logger.warning(
                    f"[AutoSwitch] è°ƒç”¨å¤±è´¥ (attempt {attempt + 1}/{max_retries}): "
                    f"model={profile.model_name}, reason={reason.value}, error={str(e)[:200]}"
                )

                # å¦‚æœæ²¡æœ‰åˆ‡æ¢å‘ç”Ÿæˆ–å·²ç»è¯•è¿‡æ‰€æœ‰æ¨¡å‹ï¼Œåœæ­¢é‡è¯•
                if not self._enabled:
                    break
                if new_id is None and len(tried_models) >= len(self._profiles):
                    break

        raise last_error


# å…¨å±€å•ä¾‹
auto_switcher = ModelAutoSwitcher()


def get_auto_switcher() -> ModelAutoSwitcher:
    """è·å–è‡ªåŠ¨åˆ‡æ¢å™¨å®ä¾‹"""
    return auto_switcher


class FailoverChatModel:
    """
    å¸¦è‡ªåŠ¨æ•…éšœè½¬ç§»çš„ LLM ä»£ç†ç±»

    åŒ…è£… browser-use çš„ ChatOpenAIï¼Œåœ¨ ainvoke è°ƒç”¨ä¸­æ‹¦æˆª 429/RateLimitErrorï¼Œ
    è‡ªåŠ¨ä» ModelAutoSwitcher è·å–ä¸‹ä¸€ä¸ªå¯ç”¨æ¨¡å‹ï¼Œåˆ›å»ºæ–°çš„ LLM å®ä¾‹å¹¶é‡è¯•ã€‚

    å¯¹ browser-use Agent å®Œå…¨é€æ˜ â€” Agent åªçœ‹åˆ°ä¸€ä¸ªæ­£å¸¸çš„ LLM å¯¹è±¡ã€‚
    """

    def __init__(self, initial_llm, switcher: ModelAutoSwitcher = None):
        # å‡å°‘åº•å±‚ LLM çš„é‡è¯•æ¬¡æ•°ï¼Œå› ä¸º FailoverChatModel è‡ªå·±å¤„ç†é‡è¯•ï¼ˆé€šè¿‡åˆ‡æ¢æ¨¡å‹ï¼‰
        if hasattr(initial_llm, 'max_retries'):
            initial_llm.max_retries = 1
        self._current_llm = initial_llm
        self._switcher = switcher or get_auto_switcher()
        self._switch_count = 0
        self._max_switches_per_call = 3  # å•æ¬¡ ainvoke æœ€å¤šåˆ‡æ¢ 3 æ¬¡

    # ---- é€ä¼  BaseChatModel åè®®æ‰€éœ€çš„å±æ€§ ----

    @property
    def model(self) -> str:
        return self._current_llm.model

    @model.setter
    def model(self, value: str):
        self._current_llm.model = value

    @property
    def provider(self) -> str:
        return getattr(self._current_llm, 'provider', 'openai')

    @property
    def name(self) -> str:
        return getattr(self._current_llm, 'name', str(self.model))

    @property
    def model_name(self) -> str:
        return self.model

    # ---- é€ä¼ å…¶ä»–å¸¸ç”¨å±æ€§ ----

    def __getattr__(self, name):
        """å°†æœªå®šä¹‰çš„å±æ€§è®¿é—®ä»£ç†åˆ°åº•å±‚ LLM"""
        return getattr(self._current_llm, name)

    # ---- æ ¸å¿ƒï¼šå¸¦æ•…éšœè½¬ç§»çš„ ainvoke ----

    async def ainvoke(self, messages, output_format=None):
        """
        å¸¦è‡ªåŠ¨æ•…éšœè½¬ç§»çš„ ainvoke

        å½“é‡åˆ° 429/RateLimitError æ—¶ï¼š
        1. é€šçŸ¥ auto_switcher æ ‡è®°å½“å‰æ¨¡å‹å¤±è´¥
        2. è·å–ä¸‹ä¸€ä¸ªå¯ç”¨æ¨¡å‹çš„é…ç½®
        3. åˆ›å»ºæ–°çš„ LLM å®ä¾‹
        4. ç”¨æ–° LLM é‡è¯•
        """
        switches_this_call = 0
        last_error = None

        while switches_this_call <= self._max_switches_per_call:
            try:
                result = await self._current_llm.ainvoke(messages, output_format)
                # æˆåŠŸ â€” æ ‡è®°æˆåŠŸ
                if self._switcher.enabled and self._switcher._current_model_id:
                    self._switcher.mark_success(self._switcher._current_model_id, 0)
                return result

            except Exception as e:
                error_msg = str(e)
                last_error = e

                # åˆ¤æ–­æ˜¯å¦ä¸ºå¯åˆ‡æ¢çš„é”™è¯¯ï¼ˆ429 / quota / rate limitï¼‰
                if not self._is_switchable_error(e, error_msg):
                    raise

                if not self._switcher.enabled:
                    logger.warning(f"[FailoverLLM] è‡ªåŠ¨åˆ‡æ¢æœªå¼€å¯ï¼Œæ— æ³•æ•…éšœè½¬ç§»: {error_msg}")
                    raise

                # æ ‡è®°å½“å‰æ¨¡å‹å¤±è´¥
                current_id = self._switcher._current_model_id
                reason = classify_failure_reason(e)
                new_id = self._switcher.mark_failure(current_id or 0, reason)

                if new_id and new_id != current_id:
                    # åˆ‡æ¢æˆåŠŸï¼Œåˆ›å»ºæ–° LLM
                    logger.info(
                        f"[FailoverLLM] ğŸ”„ æ¨¡å‹åˆ‡æ¢: ID={current_id} â†’ ID={new_id}, "
                        f"æ­£åœ¨åˆ›å»ºæ–° LLM å®ä¾‹..."
                    )
                    new_llm = self._create_llm_from_profile(new_id)
                    if new_llm:
                        self._current_llm = new_llm
                        self._switch_count += 1
                        switches_this_call += 1
                        logger.info(
                            f"[FailoverLLM] âœ… å·²åˆ‡æ¢åˆ°æ–°æ¨¡å‹ï¼Œç´¯è®¡åˆ‡æ¢ {self._switch_count} æ¬¡ï¼Œ"
                            f"æœ¬æ¬¡è°ƒç”¨ç¬¬ {switches_this_call} æ¬¡åˆ‡æ¢"
                        )
                        continue  # ç”¨æ–° LLM é‡è¯•
                    else:
                        logger.error("[FailoverLLM] âŒ åˆ›å»ºæ–° LLM å®ä¾‹å¤±è´¥")
                        raise
                else:
                    logger.warning(
                        f"[FailoverLLM] âŒ æ²¡æœ‰å¯ç”¨çš„å¤‡é€‰æ¨¡å‹ï¼Œæ— æ³•åˆ‡æ¢ "
                        f"(current={current_id}, new={new_id})"
                    )
                    raise

        # è¶…è¿‡æœ€å¤§åˆ‡æ¢æ¬¡æ•°
        logger.error(f"[FailoverLLM] âŒ å·²è¾¾åˆ°æœ€å¤§åˆ‡æ¢æ¬¡æ•° {self._max_switches_per_call}")
        if last_error:
            raise last_error

    def _is_switchable_error(self, error, error_msg: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¯ä»¥é€šè¿‡åˆ‡æ¢æ¨¡å‹è§£å†³çš„é”™è¯¯"""
        # browser-use çš„ ModelRateLimitError
        error_type = type(error).__name__
        if error_type == 'ModelRateLimitError':
            return True

        # é€šç”¨ 429 æ£€æµ‹
        lower_msg = error_msg.lower()
        if '429' in error_msg:
            return True
        if 'rate limit' in lower_msg or 'rate_limit' in lower_msg:
            return True
        if 'quota' in lower_msg and ('exceeded' in lower_msg or 'exhausted' in lower_msg):
            return True
        if 'é…é¢' in error_msg or 'é™æµ' in error_msg:
            return True

        return False

    def _create_llm_from_profile(self, model_id: int):
        """æ ¹æ® model_id ä» profile åˆ›å»ºæ–°çš„ browser-use LLM å®ä¾‹"""
        profile = self._switcher._profiles.get(model_id)
        if not profile:
            logger.error(f"[FailoverLLM] æ‰¾ä¸åˆ°æ¨¡å‹ profile: ID={model_id}")
            return None

        try:
            from llm.factory import get_browser_use_llm
            new_llm = get_browser_use_llm(
                provider=profile.provider,
                model_name=profile.model_name,
                api_key=profile.api_key,
                base_url=profile.base_url,
                temperature=0.0,
            )
            logger.info(
                f"[FailoverLLM] åˆ›å»ºæ–° LLM: provider={profile.provider}, "
                f"model={profile.model_name}"
            )
            # å‡å°‘é‡è¯•æ¬¡æ•°ï¼ŒFailoverChatModel è‡ªå·±å¤„ç†åˆ‡æ¢
            if hasattr(new_llm, 'max_retries'):
                new_llm.max_retries = 1
            return new_llm
        except Exception as e:
            logger.error(f"[FailoverLLM] åˆ›å»º LLM å¤±è´¥: {e}")
            return None

    @property
    def total_switches(self) -> int:
        return self._switch_count
